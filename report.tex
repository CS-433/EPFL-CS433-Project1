\documentclass[10pt,conference,compsocconf]{IEEEtran}
\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment

\begin{document}
\title{Project 1: Machine Learning}
\author{
  Sierra, Sandra\\
  \textit{sandra.sierravega@epfl.ch}
  \and
  Gómez, Belén\\
  \textit{belen.gomezgarcia@epfl.ch}
  \and
  Mozo, Rafael\\
  \textit{rafael.mozoarias@epfl.ch}
}
\maketitle

\begin{abstract}
    The Higgs boson is an elementary particle in the Standard Model of physics which explains why other particles have mass. The goal of the Higgs Boson Machine Learning Challenge estimate the likelihood that a given event’s signature was the result of a Higgs Boson (signal) or some other process/particle (background). In this project, we have faced the problem using different regression and clasification methods, achieving the best result using Ridge Regression with a categorical accuracy of 0.756 on AIcrowd.
\end{abstract}

\section{Introduction}
    The aim of this project is to find a model that predicts whether a Higgs boson signal or background decay after a simulated particle collision event. Given a training data with several experiments and its outcome, we will manipulate the dataset to deal with missing values, apply different methods of both regression and classification and estimate how well our method is doing. The final goal will be to find the best model that reaches the highest categorical accuracy according to AIcrowd.\\
    
\section{Exploratory data analysis}
\label{sec:structure-paper}
    After importing the training set, we observe that there is an amount of 250.000 events, 30 features and only two prediction values, 'b' for background and 's' for signal. Hence, the problem is a Binary Classification with $Y \in \lbrace b, s \rbrace$ and a proportion of  65.73\% and 34.27\% respectively.
    
    Let's now deal with missing values. According to the documentation, the value for the mass is -999.0 when the topology of the event was too far from the expected one. We can see that there are 38114 missing values for this feature (i.e. DER\_mass\_MMC). To replace this values, we are going to use the median of the rest of the values for the feature. Other option will be to use the mean, but we will stick with the first option as it is more robust when we have outliers.
    
    Regarding the other features, the missing values depend on the number of jets of the event (i.e. PRI\_jet\_num).
    \begin{itemize}
        \item If it had no jets, a specific set $S$ of the features presents missing values.
        \item If it had 1 jet, a specific subset $S' \subset S$ of the features presents missing values.
        \item If it had either 2 or 3 jets, there are no missing values.
    \end{itemize}
    In order to have this into account, we will create 3 different masks so we can create 3 different models that fit better.\\
    
    
\section{Feature processing}
    We have used a correlation table to check if there are any relationships between the features. There are some features with a correlation higher than 0.9. This value is enough to consider that there is a close correlation between them, so they don't give us new information. Hence, we remove them.\\
    
    
\section{Overfitting and underfitting}
    ***\\
    
    
\section{Methods and visualization}
    As it was required in the project description, we have implemented six regression methods in order to fit our model: Gradient Descent, Stochastic Gradient Descent, Least Squares, Ridge Regression, Logistic Regression and Regularized Logistic Regression.
   
    \subsection{Training}
    **
    
    \subsection{Visualization}
    **
    
    \subsection{testing}
    **\\
    
    
\section{Discussion}
    Surprisingly, our tests with the Logistic Regression did not produce better results than Ridge Regression both locally and on AIcrowd validation, even training with crossvalidation and a simpler train-test set subdivision. This could due to the fact that Logistic Regression is the result of an iterative method and in order to reach a good accuracy we would have had to run SGD for more iterations (we tried with 10,000 and 50,000 iterations).
    On the other hand, Ridge Regression simply solves a linear system, and does not have to solve an optimization problem. We have also encountered some unexpected behaviours in tuning hyperparameters for Ridge Regression. In fact, in some cases, with high polynomial degrees (e.g. 12) and cross-validation, both test and training set accuracies were growing, while we were expecting the test set accuracy to start decreasing after a certain polynomial degree. However, the test set RMSE was behaving as expected (increasing after degree 5), and then we chose to keep a low degree. We also tuned the lambda with both low (2) and high (12) polynomial degrees, and we noticed that with high degrees RMSE increased and accuracy decreased faster than with low degrees. We interpreted it as due to overfitting.\\

\section{Summary}
    For this project we used machine learning to make predictions of simulated particle collision events, and whether they were a Higgs boson signal or a background decays. Important steps in the process were dealing with missing values, sub-categorization, data transformation
    and testing/tuning the models. By comparing the Regularized Logistic Regression with SDG with Ridge Regression, it became evident that the latter was better suited for the task.

\end{document}
